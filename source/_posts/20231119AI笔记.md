---
title: torch batch_norm计算举例
date: 2021-05-08 18:38:29
tags: [深度学习]
---

# 1. Relu

## 1.1 优缺点

- 优点
  - 使用Relu的SGD算法的收敛速度比sigmoild快
  - 在x>0上，不会出现梯度饱和和梯度小时的问题
  - 计算复杂度低，不需要进行指数运算
- 缺点
  - Relu的输出不是0均值的，它把所有小于的0都置位0，使得所有参数的更新方向都相同，导致了zigzag现象
  - 会有神经元坏死现象
  - Relu不会对数据做幅度的压缩



## 1.2 zigzag现象（待更新，不同优化算法不同）

模型中所有的参数在一次梯度更新的过程中，<font color='red' >更新方向相同</font>。各个参数不可以出走和总体梯度下降最快的方向更新, 某一层的
$$
f =Relu(\Sigma w_ix_i+b_i) \\ 
\frac{\partial f}{\partial w_i}=x_i,x_i>0
$$
梯度(从loss回传的)
$$
\frac{\partial L}{\partial f} \frac{\partial f}{\partial w_i} =\frac{\partial L}{\partial f}x_i,x_i>0
$$
$\partial L/\partial f$ 表示该层之后的梯度（即从loss回传回来的），如果$\partial L/\partial f > 0$

