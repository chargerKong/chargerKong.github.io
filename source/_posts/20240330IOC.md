---
title: 逆向最优控制 (inverse optimal control, IOC)
date: 2024-03-30 18:38:29
tags: [自动驾驶]
---

> 参考: https://www.jos.org.cn/jos/article/pdf/6671

行为克隆的问题：

​        行为克隆方法只考虑在每个状态采取的动作与专家样本是否匹配, **不考虑未来收益**, 所以若与环境交互路径 很长且专家样本不足, 则行为克隆方法会将细微误差在连续决策中逐步放大, 甚至环境发生一点变化, 都会极大影 响算法性能, 这被称为行为克隆方法中的复合误差问题

​        在一般强 化学习问题中, 奖赏函数由人工设定, 而在许多复杂问题中, 很难设计出精确的奖赏函数, 此时却很容易通过专家 策略采样专家样本, 例如汽车驾驶[29]和操纵机器打结等[30,31] . 针对这类问题, 逆向强化学习方法**抛弃人工设定奖赏 函数过程**, 直接通过专家样本重建奖赏函数, 并依据所得奖赏函数求解最优策略, 达到模仿专家策略的目的

​        相比行为克隆方法, 逆向强化学习方法具有更好的泛化性和鲁棒性. 若环境改变或在专家样本状态-动作 空间之外, 仍可保证算法性能. 此外, 由于逆向强化学习方法通过最大化累积奖赏值求解最优策略, 所以逆向强化 学习方法方法不存在行为克隆方法中的复合误差问题

​         逆向强化学习方法的奖赏函数最初为线性函数, 由 Ng 等人于 2000 年提出[35] , 此后基于学徒学习的逆向强化 学习算法[36]、最大边际算法[37]、最大熵算法[38]和相对熵算法[39]等被相继提出. 这类算法假设状态或状态-动作对 特征的线性组合为奖赏函数, **算法的最终目标为求解每个特征的系数,** 当特征系数确定, 奖赏函数随之确定. 逆向 强化学习方法中多个奖赏函数均可求解专家策略。算法仍需在满足条件的奖赏函数集合中选择最优解。

​        基于线性奖赏函数的逆向强化学习算法虽有一些不错的结果，但是由于所有的奖赏函数都是特征的线性组合, 导致以下问题: (1) 特征需要凭借人的经验来选 取, 增加了算法的难度和不稳定性; (2) 线性奖赏函数形式简单, 存在表达能力有限的问题.

​       非线性奖赏函数的逆向强化学习算法被提出, 包括基于贝叶斯的非参数 化特征构建算法[44]和基于神经网络的非线性逆向强化学习算法. **传统逆向强化学习算法 (例如学徒学习算法、最大边际算法、最大熵算法、相对 熵算法) 与神经网络结合**, 将神经网络作为奖赏函数, 取得了很好的效果. 这类算法可以通过神经网络自动提取状 态特征, 具有更强的表征能力. 目前, 在游戏、自动驾驶、路径导航和机器控制领域取得了一定成果

# 强化学习符号

$\pi(a|s)$: 状态s到动作a的策略，策略分以下两种

- 确定性策略: 确定性策略表示在每个状态 选择的动作确定且唯一: 即$\pi: S=>A$
- 随机策略:  随机策略表示在每个状态的动作选择满足一个概率分布$\pi: S=>prob(A)$

